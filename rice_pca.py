# -*- coding: utf-8 -*-
"""Rice_PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q-5Ni-gY45-_zDUwvcUDGyZM2NgvPlQp

<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#[EDA]-rice_data-set" data-toc-modified-id="[EDA]-rice_data-set-1">[EDA] rice_data set</a></span><ul class="toc-item"><li><span><a href="#1.-data-import" data-toc-modified-id="1.-data-import-1.1">1. data import</a></span><ul class="toc-item"><li><span><a href="#1.1-데이터-정보-확인" data-toc-modified-id="1.1-데이터-정보-확인-1.1.1">1.1 데이터 정보 확인</a></span></li><li><span><a href="#1.2-중복행-제거" data-toc-modified-id="1.2-중복행-제거-1.1.2">1.2 중복행 제거</a></span></li><li><span><a href="#1.3-널값-확인" data-toc-modified-id="1.3-널값-확인-1.1.3">1.3 널값 확인</a></span></li></ul></li><li><span><a href="#2.-단변량-변수-확인" data-toc-modified-id="2.-단변량-변수-확인-1.2">2. 단변량 변수 확인</a></span><ul class="toc-item"><li><span><a href="#2.1-반응변수-분포-확인-(label)" data-toc-modified-id="2.1-반응변수-분포-확인-(label)-1.2.1">2.1 반응변수 분포 확인 (label)</a></span></li><li><span><a href="#2.2-설명변수-분포-확인" data-toc-modified-id="2.2-설명변수-분포-확인-1.2.2">2.2 설명변수 분포 확인</a></span></li></ul></li><li><span><a href="#3.-변수간-관계-파악" data-toc-modified-id="3.-변수간-관계-파악-1.3">3. 변수간 관계 파악</a></span><ul class="toc-item"><li><span><a href="#3.1-상관관계-파악" data-toc-modified-id="3.1-상관관계-파악-1.3.1">3.1 상관관계 파악</a></span></li><li><span><a href="#3.2-클래스별-단변량-변수-분포-확인" data-toc-modified-id="3.2-클래스별-단변량-변수-분포-확인-1.3.2">3.2 클래스별 단변량 변수 분포 확인</a></span></li></ul></li></ul></li><li><span><a href="#2.-[PCA]-:-2가지-패키지로-진행" data-toc-modified-id="2.-[PCA]-:-2가지-패키지로-진행-2">2. [PCA] : 2가지 패키지로 진행</a></span><ul class="toc-item"><li><span><a href="#2.0.-Data-Split" data-toc-modified-id="2.0.-Data-Split-2.1">2.0. Data Split</a></span></li><li><span><a href="#2.1.-PCA-패키지-이용" data-toc-modified-id="2.1.-PCA-패키지-이용-2.2">2.1. PCA 패키지 이용</a></span><ul class="toc-item"><li><span><a href="#2.1.1-No-scaling" data-toc-modified-id="2.1.1-No-scaling-2.2.1">2.1.1 No scaling</a></span></li><li><span><a href="#2.1.2-scaling" data-toc-modified-id="2.1.2-scaling-2.2.2">2.1.2 scaling</a></span></li><li><span><a href="#2.1.3-loading-시각화를-위한-3개-주성분으로-분석" data-toc-modified-id="2.1.3-loading-시각화를-위한-3개-주성분으로-분석-2.2.3">2.1.3 loading 시각화를 위한 3개 주성분으로 분석</a></span></li><li><span><a href="#2.1.4-biplot-시각화를-위한-3개-주성분으로-분석" data-toc-modified-id="2.1.4-biplot-시각화를-위한-3개-주성분으로-분석-2.2.4">2.1.4 biplot 시각화를 위한 3개 주성분으로 분석</a></span></li></ul></li><li><span><a href="#2.2.pca-패키지-이용" data-toc-modified-id="2.2.pca-패키지-이용-2.3">2.2.pca 패키지 이용</a></span><ul class="toc-item"><li><span><a href="#3.2.1.-no-scaling" data-toc-modified-id="3.2.1.-no-scaling-2.3.1">3.2.1. no scaling</a></span></li><li><span><a href="#3.2.2-scaling" data-toc-modified-id="3.2.2-scaling-2.3.2">3.2.2 scaling</a></span></li></ul></li></ul></li><li><span><a href="#3.-[Modeling]-Logistric-regression-for-Multi-classification" data-toc-modified-id="3.-[Modeling]-Logistric-regression-for-Multi-classification-3">3. [Modeling] Logistric regression for Multi classification</a></span><ul class="toc-item"><li><span><a href="#3.1-원본-데이터-:-로지스틱-회귀를-이용한-다중-분류" data-toc-modified-id="3.1-원본-데이터-:-로지스틱-회귀를-이용한-다중-분류-3.1">3.1 원본 데이터 : 로지스틱 회귀를 이용한 다중 분류</a></span></li><li><span><a href="#3.2-scaling-데이터-:-로지스틱-회귀를-이용한-다중-분류" data-toc-modified-id="3.2-scaling-데이터-:-로지스틱-회귀를-이용한-다중-분류-3.2">3.2 scaling 데이터 : 로지스틱 회귀를 이용한 다중 분류</a></span></li><li><span><a href="#3.3-차원축소-데이터-:-로지스틱-회귀를-이용한-다중-분류" data-toc-modified-id="3.3-차원축소-데이터-:-로지스틱-회귀를-이용한-다중-분류-3.3">3.3 차원축소 데이터 : 로지스틱 회귀를 이용한 다중 분류</a></span></li></ul></li></ul></div>
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import dataframe_image as dfi
import matplotlib as mpl
import plotly.express as px
from scipy.stats import skew, kurtosis

sns.set_palette("pastel")
sns.set_style("darkgrid")


import matplotlib.font_manager as fm
# %matplotlib inline
plt.rc('font', family='Malgun Gothic')
plt.rc('axes', unicode_minus=False) # 마이너스 폰트 설정

title_font = {
    'fontsize': 20,
    'fontweight': 'bold'
}

# 글씨 선명하게 출력하는 설정
# %config InlineBackend.figure_format = 'retina'
font = {'size'   : 15}
plt.rc('font', **font)

"""# [EDA] rice_data set

## 1. data import

**[Variables]**

![image.png](attachment:image.png)


**[Relevant Papers:]**

1: KOKLU, M., CINAR, I. and TASPINAR, Y. S. (2021). CLASSification of rice varieties with deep learning methods. Computers and Electronics in Agriculture, 187, 106285.  

DOI: https://doi.org/10.1016/j.compag.2021.106285

2: CINAR, I. and KOKLU, M. (2021). Determination of Effective and Specific Physical Features of Rice Varieties by Computer Vision In Exterior Quality Inspection. Selcuk Journal of Agriculture and Food Sciences, 35(3), 229-243.

DOI: https://doi.org/10.15316/SJAFS.2021.252

3: CINAR, I. and KOKLU, M. (2022). Identification of Rice Varieties Using Machine Learning Algorithms. Journal of Agricultural Sciences, 28 (2), 307-325.

DOI: https://doi.org/10.15832/ankutbd.862482

4: CINAR, I. and KOKLU, M. (2019). CLASSification of Rice Varieties Using Artificial Intelligence Methods. International Journal of Intelligent Systems and Applications in Engineering, 7(3), 188-194.

DOI: https://doi.org/10.18201/ijisae.2019355381
"""

import os
rice = pd.read_csv("Rice_preprocess.csv").drop("Unnamed: 0",axis=1)
rice

rice.head(5)

"""### 1.1 데이터 정보 확인"""

# 74703 rows × 17 columns
# 총 17개의 변수, class를 제외한 나머지 변수는 수치형
# 픽셀 수와 관련된 Area, Convex Area 변수의 경우 정수형, 그 외 변수의 경우 실수형 값을 가짐.
rice.info()

round(rice.describe(),2).T

"""### 1.2 중복행 제거"""

rice[rice.duplicated()] # 중복행 없음

"""### 1.3 널값 확인"""

# 결측치 존재하지 않음.
rice.isna().sum()

"""## 2. 단변량 변수 확인

### 2.1 반응변수 분포 확인 (label)
"""

print(rice.CLASS.value_counts()) # 총 5종류의 rice 존재
plt.figure(figsize=(10,6))
plt.title("rice CLASS distribution",fontdict=title_font)
sns.countplot(x='CLASS', data=rice)

plt.savefig('savefig_class.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png')
plt.show()
# 균형 데이터셋

"""### 2.2 설명변수 분포 확인"""

num_index = [f for f in rice.columns if rice[f].dtype != 'object']

skewness = rice[num_index].apply(lambda x: skew(x))
kurtosis_value = rice[num_index].apply(lambda x: kurtosis(x))
pd.DataFrame({"skew":skewness,"kurtosis":kurtosis_value})

# 왜도와 첨도 둘 다 특이사항이 있는 변수는 크게 없어 보임.

# 수치형 설명변수 개별 분포 확인
for i in range(len(num_index)):
    title_font = {'fontsize': 23, 'fontweight': 'bold'}
    plt.figure(figsize=(10,6))

    plt.subplot(121)
    plt.title(num_index[i]+" : distplot",fontdict=title_font,)
    sns.histplot(x=(rice[num_index[i]]),kde=True,color="blue")

    plt.subplot(122)
    plt.title(num_index[i]+" : boxplot",fontdict=title_font,)
    sns.boxplot(x=(rice[num_index[i]]),color="lightgray")
    plt.savefig(num_index[i]+'_hist_box.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png')
    plt.tight_layout()

"""## 3. 변수간 관계 파악

### 3.1 상관관계 파악
"""

num_index = [f for f in rice.columns if rice[f].dtype != 'object']

plt.figure(figsize=(15, 12))
plt.title("Person Correlation of Features", y = 1.05, size = 15)
sns.heatmap(rice[num_index].astype(float).corr(), linewidths = 0.1, vmax = 1.0,
square = True, linecolor = "white", annot = True, annot_kws = {"size" : 12})

plt.title("Rice Feature corr",fontsize=30, fontweight='bold')
plt.savefig('var_corr.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png')
plt.show()

# corr matrix 확인
print(rice[num_index].astype(float).corr())

"""### 3.2 클래스별 단변량 변수 분포 확인

**1) 모양관련변수**  
['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',
 'ConvexArea', 'EquivDiameter',]

콩의 크기를 나타내는 변수들의 경우 비슷한 형태를 띄는 모습을 보임.


**2) 그 외 변수**  
 ['AspectRation',
 'Eccentricity',
 'Extent',
 'Solidity',
 'roundness',
 'Compactness',
 'ShapeFactor1',
 'ShapeFactor2',
 'ShapeFactor3',
 'ShapeFactor4']
"""

num_index = [f for f in rice.columns if rice[f].dtype != 'object']

px.box(rice, x=num_index[0], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[1], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[2], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[3], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[4], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[5], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[6], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[7], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[8], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[9], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[10], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[11], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[12], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[13], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[14], y='CLASS', hover_name='CLASS')

px.box(rice, x=num_index[15], y='CLASS', hover_name='CLASS')

"""# 2. [PCA] : 2가지 패키지로 진행"""

# 사용한 Library
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from sklearn import datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from pca import pca
from sklearn.preprocessing import StandardScaler

"""## 2.0. Data Split"""

X = rice[rice.columns[0:16]]
y = rice[rice.columns[-1]]
print("설명변수 : ","\n",rice.columns[0:16].values,"\n")
print("반응변수 : ","\n", rice.columns[-1])

# Train / Test로 데이터 분류
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

# sc.fi
X_train_std = sc.fit_transform(x_train)
X_test_std = sc.transform(x_test)

"""## 2.1. PCA 패키지 이용

### 2.1.1 No scaling
"""

# Instantiate PCA
pca = PCA()

# Determine transformed features
X_train_pca = pca.fit_transform(x_train)

# Determine explained variance using explained_variance_ration_ attribute
exp_var_pca = pca.explained_variance_ratio_

# Cumulative sum of eigenvalues; This will be used to create step plot
# for visualizing the variance explained by each principal component.
cum_sum_eigenvalues = np.cumsum(exp_var_pca)

# Create the visualization plot
plt.figure(figsize=(8,6))
plt.bar(range(1,17), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
plt.step(range(1,17), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best',fontsize=10)
plt.xticks(range(1, 17))
plt.title("Explanted Variance ration plot",fontdict=title_font,)
plt.tight_layout()
plt.savefig('[PCA]No_scaling.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png',dpi=200)
plt.show()

# 주성분 별 분산에 대한 설명력 확인
pd.options.display.float_format = '{:.7f}'.format

pd.DataFrame({"exp_Var":pca.explained_variance_, "exp_Var_ratio" : pca.explained_variance_ratio_})

"""### 2.1.2 scaling"""

dat = pd.DataFrame((X_train_std), columns=x_train.columns[0:17]).set_index(y_train)

# Instantiate PCA
pca = PCA()

# Determine transformed features
X_train_pca = pca.fit_transform(dat)

# Determine explained variance using explained_variance_ration_ attribute
exp_var_pca = pca.explained_variance_ratio_

# Cumulative sum of eigenvalues; This will be used to create step plot
# for visualizing the variance explained by each principal component.
cum_sum_eigenvalues = np.cumsum(exp_var_pca)

# Create the visualization plot
plt.figure(figsize=(8,6))
plt.bar(range(1,17), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
plt.step(range(1,17), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best',fontsize=10)
plt.xticks(range(1, 18))
plt.title("Explanted Variance ration plot",fontdict=title_font,)
plt.tight_layout()
plt.savefig('[PCA]scaling.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png',dpi=200)
plt.show()

# 주성분 별 분산에 대한 설명력 확인
pd.options.display.float_format = '{:.7f}'.format

pd.DataFrame({"exp_Var":pca.explained_variance_, "exp_Var_ratio" : pca.explained_variance_ratio_})

import plotly.express as px
from sklearn.decomposition import PCA

df = pd.DataFrame((X_train_std), columns=x_train.columns[0:17]).set_index(y_train)
features = df.columns

pca = PCA()
components = pca.fit_transform(df)
labels = {
    str(i): f"PC {i+1} ({var:.1f}%)"
    for i, var in enumerate(pca.explained_variance_ratio_ * 100)
}

fig = plt.figure(figsize=(10, 7))
fig = px.scatter_matrix(
    components,
    labels=labels,
    dimensions=range(3),
    color=df.index
)
fig.update_traces(diagonal_visible=True)

# 그림의 크기 조정
fig.update_layout(width=1024, height=800)
fig.show()

import plotly.express as px
from sklearn.decomposition import PCA

df = pd.DataFrame((X_train_std), columns=x_train.columns[0:17]).set_index(y_train)
features = df.columns

pca = PCA()
components = pca.fit_transform(df)
labels = {
    str(i): f"PC {i+1} ({var:.1f}%)"
    for i, var in enumerate(pca.explained_variance_ratio_ * 100)
}

fig = plt.figure(figsize=(10, 7))
fig = px.scatter_matrix(
    components,
    labels=labels,
    dimensions=range(4),
    color=df.index
)
fig.update_traces(diagonal_visible=True)

# 그림의 크기 조정
fig.update_layout(width=1024, height=800)
fig.show()

"""### 2.1.3 loading 시각화를 위한 3개 주성분으로 분석"""

import plotly.express as px
from sklearn.decomposition import PCA

df = pd.DataFrame((X_train_std), columns=x_train.columns[0:17]).set_index(y_train)
features = df.columns

pca = PCA(n_components=3)
components = pca.fit_transform(df,3)
total_var = pca.explained_variance_ratio_.sum() * 100

fig = px.scatter_3d(
    components, x=0, y=1, z=2, color=df.index,
    title=f'Total Explained Variance: {total_var:.2f}%',
    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},
    size = np.repeat(50, len(df))
)
fig.update_layout(width=1024, height=800)
fig.show()

# 주성분 별 분산에 대한 설명력 확인
pd.DataFrame({"exp_Var":pca.explained_variance_, "exp_Var_ratio" : pca.explained_variance_ratio_})

"""### 2.1.4 biplot 시각화를 위한 3개 주성분으로 분석"""

plt.style.use('default')

features = df.columns
pca = PCA(n_components=3)

# Fit and transform data
pca_features = pca.fit_transform(df)

# Create dataframe
pca_df = pd.DataFrame(
    data=pca_features,
    columns=['PC1', 'PC2', 'PC3'])

# Apply the targett names
pca_df['target'] = df.index

# map target names to PCA features
target_names = {
    'Arborio':0,
    'Jasmine':1,
    'Basmati':2,
    'Ipsala':3,
    'Karacadag':4
}

# Apply the targett names
pca_df['target'] = df.index
pca_df['target'] = pca_df['target'].map(target_names)


# Create the scaled PCA dataframe
pca_df_scaled = pca_df.copy()

scaler_df = pca_df[['PC1', 'PC2', 'PC3']]
scaler = 1 / (scaler_df.max() - scaler_df.min())

for index in scaler.index:
    pca_df_scaled[index] *= scaler[index]


# Initialize the 3D graph
fig = plt.figure(figsize=(8,8))
ax = fig.add_subplot(111, projection='3d')

# Define scaled features as arrays
xdata = pca_df['PC1']
ydata = pca_df['PC2']
zdata = pca_df['PC3']


# Plot 3D scatterplot of PCA
scatter = ax.scatter3D(
    xdata,
    ydata,
    zdata,
    c=pca_df['target'],
    cmap='jet',
    s=100,
    alpha=0.7,edgecolor="white")

# Define the x, y, z variables
loadings = pca.components_ # 임의로 25를 곱함.
xs = loadings[0]*25
ys = loadings[1]*25
zs = loadings[2]*25

# Plot the arrows
x_arr = np.zeros(len(loadings[0]))
y_arr = z_arr = x_arr
ax.quiver(x_arr, y_arr, z_arr, xs, ys, zs,arrow_length_ratio=0.1)


# Plot the loadings
for i, varnames in enumerate(features):
    ax.scatter(xs[i], ys[i], zs[i], s=150)
    ax.text(
        xs[i] + 0.01,
        ys[i] + 0.01,
        zs[i] + 0.01,
        varnames, fontweight='bold')

legend_labels = list(target_names.keys())
ax.legend(handles=scatter.legend_elements()[0], labels=legend_labels, ncol=5,
          loc="upper center", bbox_to_anchor=(0.5,0),fontsize=12)

# Plot title of graph
# plt.title(f'3D Biplot of Rice_PCA',fontsize=25, fontweight='bold')
# Plot x, y, z labels

ax.set_xlabel('Principal component 1', rotation=150)
ax.set_ylabel('Principal component 2')
ax.set_zlabel('Principal component 3', rotation=60)
plt.tight_layout()

plt.savefig('[PCA]NOTT_3d_scaling_biplot.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png',dpi=200)
plt.show()

# loading 크기 확인
print(sum(loadings[0]**2),end=' ')
print(sum(loadings[1]**2),end=' ')
print(sum(loadings[2]**2),end=' ')

# 주성분 별 분산에 대한 설명력 확인
pd.DataFrame({"exp_Var":pca.explained_variance_, "exp_Var_ratio" : pca.explained_variance_ratio_})

"""## 2.2.pca 패키지 이용

### 3.2.1. no scaling
"""

from pca import pca
# Initialize and keep all PCs
model = pca()
# Fit transform
out = model.fit_transform(X)

# Print the top features.
print(out['topfeat'])

# 이상치 관련 결과는 따로 찾아보기

out["loadings"].T

print(out["explained_var"])
print(out["explained_var"].sum())
print(out["variance_ratio"])
print(out["pcp"])

pd.DataFrame({'exp_var':out["explained_var"],"exp_var_Rat":out["variance_ratio"]})

model.plot()
plt.savefig('[pca]noscaling_cumulat_expvar.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png',dpi=200)
plt.show()

"""### 3.2.2 scaling"""

dat = pd.DataFrame((X_train_std), columns=x_train.columns[0:17]).set_index(y_train)

model = pca()

# Fit transform
out = model.fit_transform(dat)

# Print the top features.
print(out['topfeat'])

out["loadings"].T

print("전체 PC 설명된 분산 : ",out["explained_var"],"\n")
print("총 분산 : ",out["explained_var"].sum(),"\n")

print("전체 PC 설명된 총 분산비율 : ",out["variance_ratio"],"\n")
print("2PC 설명된 총 분산비율 : ",out["variance_ratio"][0:2].sum(),"\n")
print("3PC 설명된 총 분산비율 : ",out["variance_ratio"][0:3].sum(),"\n")
print("4PC 설명된 총 분산 : ",out["pcp"],"\n")

pd.DataFrame({'exp_var':out["explained_var"],"exp_var_Rat":out["variance_ratio"]})

model.plot()
plt.savefig('[pca]scaling_cumulat_expvar.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png',dpi=200)
plt.show()

# Use custom cmap for classlabels (as an example I explicitely provide three colors).
# cmap_light = mpl.colors.ListedColormap(['lightgreen', 'lightpink', 'lightblue', 'lightorange', 'lightyellow'])
model.biplot(cmap="coolwarm")
plt.savefig('[pca]biplot_scaling_cumulat_expvar.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png',dpi=200)

# Use custom cmap for classlabels (as an example I explicitely provide three colors).
# cmap_light = mpl.colors.ListedColormap(['lightgreen', 'lightpink', 'lightblue', 'lightorange', 'lightyellow'])
model.biplot(cmap="coolwarm")
plt.savefig('[pca]biplot2_scaling_cumulat_expvar.png',
            facecolor='#eeeeee',
            edgecolor='gray',
            format='png',dpi=200)

"""# 3. [Modeling] Logistric regression for Multi classification

## 3.1 원본 데이터 : 로지스틱 회귀를 이용한 다중 분류
"""

import time
start_time = time.time()

clf_ori = LogisticRegression(max_iter=1000, random_state=0,
                        multi_class='multinomial',
                        solver='sag')
clf_ori.fit(x_train, y_train)

end_time = time.time()
execution_time1 = end_time - start_time

execution_time1
# 모델이 수렴하지않으며 시간 또한 매우 많이 소요

# 예측 및 결과 확인
# 예측 및 결과 확인
pred_tr = clf_ori.predict(x_train)
pred_te = clf_ori.predict(x_test)

print(accuracy_score(y_train, pred_tr))
print(accuracy_score(y_test, pred_te))

"""## 3.2 scaling 데이터 : 로지스틱 회귀를 이용한 다중 분류"""

import time
start_time = time.time()

clf_ori = LogisticRegression(max_iter=1000, random_state=0,
                        multi_class='multinomial',
                        solver='sag')
clf_ori.fit(X_train_std, y_train)

end_time = time.time()
execution_time2 = end_time - start_time

execution_time2

# 예측 및 결과 확인
# 예측 및 결과 확인
pred_tr = clf_ori.predict(X_train_std)
pred_te = clf_ori.predict(X_test_std)

print(accuracy_score(y_train, pred_tr))
print(accuracy_score(y_test, pred_te))

"""## 3.3 차원축소 데이터 : 로지스틱 회귀를 이용한 다중 분류"""

df_Tr = pd.DataFrame((X_train_std), columns=x_train.columns[0:17])
df_Te = pd.DataFrame((X_test_std), columns=x_train.columns[0:17])
features = df.columns

pca = PCA(n_components=3)
components_Tr = pca.fit_transform(df_Tr,3)
components_Te = pca.transform(df_Te)

# 모델 생성 / 최대 iter 2000 / 다중 분류이므로 multi_class 적용
import time
start_time = time.time()

clf = LogisticRegression(max_iter=2000, random_state=0,
                        multi_class='multinomial',
                        solver='sag')
clf.fit(components_Tr, y_train)

end_time = time.time()
execution_time3 = end_time - start_time

execution_time3

# 예측 및 결과 확인
pred_tr = clf.predict(components_Tr)
pred_te = clf.predict(components_Te)

print(accuracy_score(y_train, pred_tr))
print(accuracy_score(y_test, pred_te))